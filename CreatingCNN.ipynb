{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Dataset\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fTJoiFSTip50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Directory\n",
        "\n",
        "To label the bird species, we need to make folders for each. As this code is being run on Google Colab, it was simple to download all the data onto Google Drive to save resources."
      ],
      "metadata": {
        "id": "FgfgNOpBi580"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As there is a lot of data that needs to be gathered, rather than doing it by hand, we can automate the process using code. Credits to [Jeremy Howard's](https://www.kaggle.com/code/jhoward/is-it-a-bird-creating-a-model-from-your-own-data) tutorial for this method of gathering data. To begin, we need to install the `duckduckgo_search` library using pip:"
      ],
      "metadata": {
        "id": "52yPr4A0CM1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install duckduckgo_search"
      ],
      "metadata": {
        "id": "2BVkz14uCKAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3Un683oBk1d"
      },
      "outputs": [],
      "source": [
        "bird_classes = ['Blackbird', 'Blackcap', 'Blue Tit', 'Bullfinch', 'Carrion Crow', 'Chaffinch', 'Chiffchaff', 'Coal Tit', 'Collared Dove', 'Common Linnet', 'Common Sandpiper', 'Common Whitethroat', 'Crossbill', 'Dotterel', 'Dunnock', 'Eurasian Jay', 'Eurasian Magpie', 'Eurasian Teal', 'Feral Pigeon', 'Fieldfare', 'Firecrest', 'Garden Warbler', 'Goldcrest', 'Golden Plover', 'Goldfinch', 'Great Spotted Woodpecker', 'Great Tit', 'Green Woodpecker', 'Greenfinch', 'Grey Wagtail', 'Hawfinch', 'House Martin', 'House Sparrow', 'Jackdaw', 'Lesser Redpoll', 'Lesser Spotted Woodpecker', 'Lesser Whitethroat', 'Long-tailed Tit', 'Meadow Pipit', 'Mealy Redpoll', 'Mistle Thrush', 'Nightingale', 'Nuthatch', \"Pallas's Warbler\", \"Pied Flycatcher\", 'Pied Wagtail', 'Redstart', 'Redwing', 'Reed Bunting', 'Robin', 'Sand Martin', 'Sedge Warbler', 'Siskin', 'Skylark', 'Song Thrush', 'Spotted Flycatcher', 'Starling', 'Stock Dove', 'Stonechat', 'Swallow', 'Swift', 'Treecreeper', 'Waxwing', 'Wheatear', 'Whinchat', 'Willow Warbler', 'Wood Warbler', 'Woodcock', 'Woodpigeon', 'Wren', 'Yellow-browed warbler']\n",
        "raw_bird_classes = "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for i in range(len(bird_classes)):\n",
        "  path = f'/content/drive/MyDrive/BirdClassifierDataset/train/{bird_classes[i]}'\n",
        "  os.mkdir(path)\n",
        "  path = f'/content/drive/MyDrive/BirdClassifierDataset/test/{bird_classes[i]}'\n",
        "  os.mkdir(path)\n",
        "  path = f'/content/drive/MyDrive/BirdClassifierDataset/valid/{bird_classes[i]}'\n",
        "  os.mkdir(path)"
      ],
      "metadata": {
        "id": "VbDca1dsi9eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Images\n",
        "For this process, we utilise the `fastcore.all`, `duckduckgo_search`,`fastdownload` and `time` libraries. Two lists were created; `bird_classes` and `raw_bird_classes` where the latter omits spaces to name the files. \n",
        "\n",
        "The `search_images()` function is defined, which scrapes the web, using the `duckduckgo_search` library, for a provided search term and returns a list of urls for the images.\n",
        "\n",
        "Then, each bird species on the list is iterated through as a search term, and 60 of the images are downloaded onto Google Drive, using the `download_url()` function. This process is in a 'try', 'except', 'else' block because sometimes the images cannot be properly downloaded and this avoids having to restart the process every time at the expense of having an inconsistent amount of images throughout the classes (not a big deal).\n",
        "\n",
        "It is important to include a `sleep(1)` statement in between each iteration to prevent the server from overloading.\n",
        "\n"
      ],
      "metadata": {
        "id": "vG0uv_PPj8-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from duckduckgo_search import ddg_images\n",
        "from fastcore.all import *\n",
        "from fastdownload import download_url\n",
        "from time import sleep\n",
        "from PIL import Image, ImageFile\n",
        "import os\n",
        "\n",
        "# grabs urls\n",
        "def search_images(term, max_images=200):\n",
        "    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n",
        "\n",
        "i = 0\n",
        "while i < len(bird_classes):\n",
        "  species = bird_classes[i]\n",
        "  raw_species = raw_bird_classes[i]\n",
        "  urls = search_images(species + 'bird', max_images=210)\n",
        "  print(species)\n",
        "  print(f'i = {i}')\n",
        "\n",
        "  j = 0\n",
        "  while j < 210: # because there will be 15 images per species\n",
        "    print(j)\n",
        "    if j < 160: path = '/content/drive/MyDrive/BirdClassifierDataset/train/' + f'{species}/{raw_species}{j}.jpg'\n",
        "    if 159 < j < 190: path = '/content/drive/MyDrive/BirdClassifierDataset/valid/' + f'{species}/{raw_species}{j}.jpg'\n",
        "    if 189 < j < 210: path = '/content/drive/MyDrive/BirdClassifierDataset/test/' + f'{species}/{raw_species}{j}.jpg'\n",
        "    try:\n",
        "      download_url(urls[j], path, show_progress=False) # downloads the images to the path\n",
        "    except:\n",
        "      print(f'Error downloading: {raw_species}{j}.jpg')\n",
        "      j += 1\n",
        "    else:\n",
        "      j += 1\n",
        "    \n",
        "    sleep(1) # prevents server from overloading\n",
        "  \n",
        "  i += 1"
      ],
      "metadata": {
        "id": "bfJXsDKEkSuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes the images being downloaded are not the ones that you intended. At this step, it is important to go through your dataset and make sure that the images being downloaded were correct.\n",
        "\n",
        "The images may be corrupted or in the incorrect format. Now we have to verify all the images to make sure they are good, otherwise the classifier will throw an error."
      ],
      "metadata": {
        "id": "4YYCObWDihrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making sure all the folders contain a suitable amount of images:"
      ],
      "metadata": {
        "id": "ESJX1HW0Dq1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def check_folder_filenum(parent_dir):\n",
        "  folder_list = os.listdir(parent_dir)\n",
        "  empty_folders = []\n",
        "  for folder in folder_list:\n",
        "    folder_path = os.path.join(parent_dir, folder)\n",
        "    folder_contents = os.listdir(folder_path)\n",
        "    if (len(folder_contents)) == 0:\n",
        "      print(f\"{folder} IS EMPTY\")\n",
        "      empty_folders.append(folder)\n",
        "    else:\n",
        "        print(f\"{folder} has {len(folder_contents)} files\")\n",
        "\n",
        "  return empty_folders\n",
        "\n",
        "i = 0\n",
        "empty_folders = []\n",
        "for i in range(3):\n",
        "  dir = \"\"\n",
        "  if i == 0: dir = \"train\"\n",
        "  if i == 1: dir = \"test\"\n",
        "  if i == 2: dir = \"valid\"\n",
        "\n",
        "  print(f\"Checking {dir} directory: \\n\")\n",
        "\n",
        "\n",
        "  empty_folders.append(check_folder_filenum(f'/content/drive/MyDrive/BirdClassifierDataset/{dir}'))\n",
        "\n",
        "print(\"Empty train folders:\\n\")\n",
        "print(empty_folders[0])\n",
        "\n",
        "print(\"\\n\\nEmpty test folders:\\n\")\n",
        "print(empty_folders[1])\n",
        "\n",
        "print(\"\\n\\nEmpty valid folders:\\n\")\n",
        "print(empty_folders[2])"
      ],
      "metadata": {
        "id": "ivO-mLIkDpsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifying the files (Link to code source: https://stackoverflow.com/a/70363370):"
      ],
      "metadata": {
        "id": "vvVjQKTaDvMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import imghdr\n",
        "\n",
        "def check_images(parent_dir, ext_list):\n",
        "    bad_images=[]\n",
        "\n",
        "    folder_list = os.listdir(parent_dir) # s_ list  = folder_list\n",
        "    for folder in folder_list:\n",
        "        folder_path = os.path.join (parent_dir, folder)\n",
        "        print ('processing class directory ', folder)\n",
        "        if os.path.isdir(folder_path): # if the path exists\n",
        "            file_list = os.listdir(folder_path)\n",
        "            for f in file_list:               \n",
        "                f_path = os.path.join (folder_path, f)\n",
        "                f_type = imghdr.what(f_path) # type of file extension\n",
        "                if ext_list.count(f_type) == 0: # checks to see if the ext is part of our list\n",
        "                  bad_images.append(f_path)\n",
        "                if os.path.isfile(f_path):\n",
        "                    try:\n",
        "                        img = cv2.imread(f_path) # tries to open file\n",
        "                        shape = img.shape # tries to get file size\n",
        "                    except:\n",
        "                        print('file ', f_path, ' is not a valid image file')\n",
        "                        bad_images.append(f_path)\n",
        "                else:\n",
        "                    print('*** fatal error, you a sub directory ', f, ' in class directory ', folder_list)\n",
        "        else:\n",
        "            print ('*** WARNING*** you have files in ', parent_dir, ' it should only contain sub directories')\n",
        "    return bad_images\n",
        "\n",
        "i = 0\n",
        "for i in range(3):\n",
        "  corrupted_files = []\n",
        "\n",
        "  dir = \"\"\n",
        "  if i == 0: dir = \"train\"\n",
        "  if i == 1: dir = \"test\"\n",
        "  if i == 2: dir = \"valid\"\n",
        "  print(f'Checking {dir} dataset: ')\n",
        "  source_dir =f'/content/drive/MyDrive/BirdClassifierDataset/{dir}' # changes for each folder\n",
        "  good_exts=['jpg', 'png', 'jpeg', 'gif', 'bmp' ] # list of acceptable extensions\n",
        "  bad_file_list = check_images(source_dir, good_exts)\n",
        "  if len(bad_file_list) !=0:\n",
        "    print('removing improper files')\n",
        "    for f in bad_file_list:\n",
        "      try:\n",
        "        print(f\"removing {f}\")\n",
        "        os.remove(f)\n",
        "      except:\n",
        "        print(f\"error removing {f}\")\n",
        "  else:\n",
        "    print(' no improper image files were found')\n",
        "\n",
        "# removing the images\n",
        "\n",
        "for dir in corrupted_files:\n",
        "  for f in dir:\n",
        "    try:\n",
        "      print(f'removing {f}')\n",
        "      os.remove(f)\n",
        "    except:\n",
        "      print(f'Error removing {f}')"
      ],
      "metadata": {
        "id": "K_omWJcUDmRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Model"
      ],
      "metadata": {
        "id": "CLhVbebeFNVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers, optimizers, metrics\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.utils import image_dataset_from_directory, to_categorical"
      ],
      "metadata": {
        "id": "7kcw6P3rE7dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we load the pre-trained InceptionV3 model with iNaturalist 2017 weights:"
      ],
      "metadata": {
        "id": "r11hdAzuFWuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "base_model = hub.KerasLayer(\"https://tfhub.dev/google/inaturalist/inception_v3/feature_vector/5\",\n",
        "               trainable=True, arguments=dict(batch_norm_momentum=0.997))"
      ],
      "metadata": {
        "id": "prZxHYnkERf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Freezing the layers from training:"
      ],
      "metadata": {
        "id": "fb5nb3LoFeIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = False"
      ],
      "metadata": {
        "id": "__HuZ9qeETK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a limited dataset, we can introduce more variety in the data by including a data augumentation model to randomly flip and rotate the images:"
      ],
      "metadata": {
        "id": "yPOYyBdTFqC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = Sequential([\n",
        "  layers.RandomFlip('horizontal'),\n",
        "  layers.RandomRotation(0.2),\n",
        "])"
      ],
      "metadata": {
        "id": "SbyqBayaFn1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will have to put the model together, while adding the classification layers:\n"
      ],
      "metadata": {
        "id": "eILs7AvtFrsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_incep = Sequential()\n",
        "model_incep.add(data_augmentation)\n",
        "\n",
        "model_incep.add(layers.Rescaling(1./255))\n",
        "\n",
        "model_incep.add(base_model)\n",
        "model_incep.add(layers.Flatten())\n",
        "\n",
        "model_incep.add(layers.Dropout(0.2))\n",
        "model_incep.add(layers.BatchNormalization())\n",
        "model_incep.add(layers.Dense(71, activation = 'softmax'))"
      ],
      "metadata": {
        "id": "GJAz1HwOEUun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_incep.summary()"
      ],
      "metadata": {
        "id": "iVueK1EvEVto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "bGT-RM_wGB8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "traindata = image_dataset_from_directory(\n",
        "                                        directory= '/content/drive/MyDrive/SmallBirdsDataset/train',\n",
        "                                        labels= 'inferred', #one_hot_train.tolist()\n",
        "                                        label_mode= 'categorical',\n",
        "                                        image_size=(224, 224),\n",
        "                                        shuffle= True,\n",
        "                                        ) \n",
        "validdata = image_dataset_from_directory(\n",
        "                                        directory= '/content/drive/MyDrive/SmallBirdsDataset/valid',\n",
        "                                        labels= 'inferred', \n",
        "                                        label_mode= 'categorical',\n",
        "                                        shuffle= True,\n",
        "                                        image_size=(224, 224),\n",
        "                                        )\n",
        "\n",
        "testdata = image_dataset_from_directory(\n",
        "                                        directory= '/content/drive/MyDrive/SmallBirdsDataset/test',\n",
        "                                        labels= 'inferred', \n",
        "                                        label_mode= 'categorical',\n",
        "                                        shuffle= True,\n",
        "                                        image_size=(224, 224),\n",
        "                                        )"
      ],
      "metadata": {
        "id": "IKTRhoc-EMxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we load the dataset.\n",
        "\n",
        "`keras.utils.image_dataset_from_directory()` loads the image and stores it as a tuple: `(image, label)`, with the image being converted into an image tensor (data structure like a matrix).\n",
        "\n",
        "### Parameters\n",
        "`labels='inferred'`\n",
        "\n",
        "The class labels are inferred from the name of the folders/structure of directory.\n",
        "\n",
        "`label_mode='categorical'`\n",
        "\n",
        "Describes the encoding of the labels. In this case we set it as categorical meaning they are encoded as a categorical vector. This allows us to apply the categorical crossentropy loss function.\n",
        "\n",
        "`image_size=(224, 224)`\n",
        "\n",
        "Sets the target image size as 224px by 224px. This is important because the ResNet50 model was trained on images of this size, and this will allow us to optimise performance.\n",
        "\n",
        "`shuffle=True`\n",
        "\n",
        "Shuffles the images in the dataset."
      ],
      "metadata": {
        "id": "jhmbI_BhGMA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the model, we need two apply two main methods: `Model.compile()` and `Model.fit()`. The former prepares the model for training, while the latter trains the model for a set number of epochs (iterations).\n",
        "\n",
        "First we start with the `Model.compile()` method. As this is a supervised learning problem with more than two classes, we set the loss function as `categorical_crossentropy`. This will ensure that each input passed into the model will be fitted into one of the specified classes, and provide the probability that the input is a member of the class.\n",
        "\n",
        "We set out optimizer as [`Adam`](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) with the `learning_rate` parameter set to 0.01. As we want to evaluate the accuracy of the model while its being trained, we set the `metrics=['accuracy']`. \n"
      ],
      "metadata": {
        "id": "wlfJj1mKGDw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_incep.compile(loss='categorical_crossentropy', \n",
        "                  optimizer= optimizers.Adam(learning_rate=0.01),\n",
        "                  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "2dCrlHASEXtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L0YBIKlT_So"
      },
      "source": [
        "After compiling, we move on to training the model. We call the `Model.fit()` method and set the training data and the validation data. We set `shuffle` as true, and set the number of epochs to be 10, with a verbose as 1, which gives us a progress bar to visualise the speed of training. Finally we save the trained weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_incep.fit(traindata,\n",
        "               validation_data= validdata,\n",
        "               shuffle= True,\n",
        "               batch_size = 32,\n",
        "               epochs=10,\n",
        "               verbose=1)\n",
        "\n",
        "model_incep.save('/content/drive/MyDrive/ML_Models/birdClassifierDIYv0/inceptiNatv0.h5')"
      ],
      "metadata": {
        "id": "TvefW2JwEZj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning"
      ],
      "metadata": {
        "id": "je1k5DGoEcLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fine tune, we load the model we just trained:"
      ],
      "metadata": {
        "id": "-p8lvhd2GzsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = load_model(\n",
        "       '/content/drive/MyDrive/ML_Models/birdClassifierDIYv0/inceptiNatv0.h5',\n",
        "       custom_objects={'KerasLayer':hub.KerasLayer(\"https://tfhub.dev/google/inaturalist/inception_v3/feature_vector/5\")}\n",
        ")"
      ],
      "metadata": {
        "id": "Y8KygaB_Ee7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we set the InceptionV3 layers as trainable so we can fine-tune the weights:"
      ],
      "metadata": {
        "id": "vENS-xl8G9Nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft.layers[2].trainable = True\n",
        "print(model_ft.summary())"
      ],
      "metadata": {
        "id": "fY109fVwEh2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we compile and fit, reducing the learning rate and number of epochs."
      ],
      "metadata": {
        "id": "PbctSLVNHCub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft.compile(loss='categorical_crossentropy', \n",
        "                  optimizer= optimizers.Adam(learning_rate=0.00001),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "history = model_ft.fit(traindata,\n",
        "               validation_data= validdata,\n",
        "               shuffle= True,\n",
        "               batch_size = 32,\n",
        "               epochs=5,\n",
        "               verbose=1)\n",
        "\n",
        "model_ft.save('/content/drive/MyDrive/ML_Models/birdClassifierDIYv0/fttiNatFTUNEv0.h5')"
      ],
      "metadata": {
        "id": "dLReeBtnEjUb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}